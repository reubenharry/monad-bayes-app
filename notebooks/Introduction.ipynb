{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "import Control.Monad.Bayes.Class\n",
    "import Control.Monad.Bayes.Enumerator\n",
    "import Control.Monad.Bayes.Sampler\n",
    "import Control.Monad\n",
    ":load Plotting.hs\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Monad-Bayes\n",
    "\n",
    "This serves as an interactive alternative to [the user guide](https://monad-bayes.netlify.app/probprog.html).\n",
    "\n",
    "We'll start by defining a probabilistic model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "model :: MonadInfer m => Double -> m Double\n",
    "model observation = do\n",
    "    mean <- uniformD [-1, 0, 1]\n",
    "    factor (normalPdf mean 1 observation)\n",
    "    return mean"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea of monad-bayes, and probabilistic programming languages in general is to define distributions as programs. `model` corresponds to the distribution that you would express mathematically as:\n",
    "\n",
    "$$ P(m | o) = \\frac{P(m)P(o|m)}{P(o)} = \\frac{1 * \\mathbb{N}(o; m, 1)}{\\sum_m TODO } $$\n",
    "TODO \n",
    "\n",
    "To orient you on the relationship between the mathematical view of a distribution and the programming one, here are some notes:\n",
    "\n",
    "- a distribution over values of type `a` has type `MonadInfer m => m a`\n",
    "- a joint distribution over values of types `a` and `b` is a distribution over a tuple: `MonadInfer m => m (a, b)`\n",
    "- a conditional distribution over values of type `a` conditioned on values of type `b` is a function into a distribution: `MonadInfer m => b -> m a`\n",
    "\n",
    "Given a value of `o`, you can think of `model` as doing the following:\n",
    "\n",
    "- first draw from the prior over possible values of `mean` (that's the line `mean <- uniformD [-1, 0, 1]`)\n",
    "- then score a draw higher according to the likelihood placed on the observation by a normal with $\\mu$=`mean`\n",
    "- then return the `mean`\n",
    "\n",
    "\n",
    "For example, if the value observed is $0.3$, then we can calculate the distribution over the mean:\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "inferredDistribution = enumerate $ model 0.3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "plotVega inferredDistribution"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.vegalite.v4+json": {
       "height": 200,
       "data": {
        "values": [
         {
          "Y": 0.198111610867497,
          "X": "-1.0"
         },
         {
          "Y": 0.44090549839518783,
          "X": "0.0"
         },
         {
          "Y": 0.36098289073731515,
          "X": "1.0"
         }
        ]
       },
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "encoding": {
        "y": {
         "type": "quantitative",
         "field": "Y"
        },
        "x": {
         "type": "nominal",
         "field": "X"
        }
       },
       "mark": "bar",
       "width": 200
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To produce this plot, we performed *inference*, to obtain the exact form of the distribution represented by `model`. Because the only random variable in `model` had a support that was small and discrete (the set $\\{-1, 0, 1\\}$), performing this inference exactly was straightforward.\n",
    "\n",
    "`enumerate` is the exact inference method offered by monad-bayes.\n",
    "\n",
    "You are encouraged to change `model` in a number of ways and observe how the results change:\n",
    "- try changing the prior (currently `uniformD [-1, 0, 1]`)\n",
    "- try changing the score (currently `factor (normalPdf mean 1 observation)`)\n",
    "- try changing the types of the observation and latent variable (i.e. `mean`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are familiar with Haskell, then it should be clear that the class of distributions you can express in this way is very broad, since we have monadic control flow. For example, you could build:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "lengthDist observations = do\n",
    "    clusterAssignments <- mapM model observations\n",
    "    return $ (> 4) $ length $ filter (==1) clusterAssignments"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a set of observations, this is the distribution over whether the number of observations which belong to the mean $1$ cluster is more or less than $4$. Consider the hassle of defining this with an equation, and you'll see why probabilistic programming is appealing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "observations <- sampleIO $ replicateM 9 $ normal 0 1\n",
    "plotVega $ enumerate $ lengthDist observations"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.vegalite.v4+json": {
       "height": 200,
       "data": {
        "values": [
         {
          "Y": 0.49915741645359685,
          "X": "False"
         },
         {
          "Y": 0.5008425835465599,
          "X": "True"
         }
        ]
       },
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "encoding": {
        "y": {
         "type": "quantitative",
         "field": "Y"
        },
        "x": {
         "type": "nominal",
         "field": "X"
        }
       },
       "mark": "bar",
       "width": 200
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exact sampling is pretty limited. For models with continuous random variables, or large discrete ones, it is a no-go. \n",
    "\n",
    "The broader goal is to be able to define your distribution of interest, like `model`, and then apply different inference technique, usually approximate, to it. This is what monad-bayes (and other probabilistic programming languages) enable. See the following tutorials for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [],
   "outputs": [
    {
     "output_type": "error",
     "ename": "",
     "evalue": "",
     "traceback": [
      "<interactive>:1:1: error: Variable not in scope: kk"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}